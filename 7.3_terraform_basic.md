# 7.3. Основы и принцип работы Терраформ

# Задание №1 Создадим бэкэнд в S3 (я делал в YC у меня Yandex Object Storage)

Если в рамках предыдущего задания у вас уже есть аккаунт AWS, то давайте продолжим знакомство со взаимодействием терраформа и aws.

1. Создайте s3 бакет, iam роль и пользователя от которого будет работать терраформ. Можно создать отдельного пользователя, а можно использовать созданного в рамках предыдущего задания, просто добавьте ему необходимы права, как описано здесь.
2. Зарегистрируйте бэкэнд в терраформ проекте как описано по ссылке выше.

![Снимок экрана от 2022-07-16 11-45-08](https://user-images.githubusercontent.com/93032289/179349541-74eae3ac-b62d-4e9e-a37d-3b7f073cf980.png)

# Задание №2 Инициализируем проект и создаем воркспейсы.


1. Выполните terraform init:
  - если был создан бэкэнд в S3, то терраформ создат файл стейтов в S3 и запись в таблице dynamodb.
  - иначе будет создан локальный файл со стейтами.
2. Создайте два воркспейса stage и prod.
3. В уже созданный aws_instance добавьте зависимость типа инстанса от вокспейса, что бы в разных ворскспейсах использовались разные instance_type.
4. Добавим count. Для stage должен создаться один экземпляр ec2, а для prod два.
5. Создайте рядом еще один aws_instance, но теперь определите их количество при помощи for_each, а не count.
6. Что бы при изменении типа инстанса не возникло ситуации, когда не будет ни одного инстанса добавьте параметр жизненного цикла create_before_destroy = true в один из рессурсов aws_instance.
7. При желании поэкспериментируйте с другими параметрами и рессурсами.

   В виде результата работы пришлите:

  -  Вывод команды terraform workspace list.
  -  Вывод команды terraform plan для воркспейса prod.

![Снимок экрана от 2022-07-16 12-51-01](https://user-images.githubusercontent.com/93032289/179351156-6fa1deda-4455-497f-ab26-9da51c088b2d.png)

После terraform plan появилось очень длинная запись:



```
hummer@hummer-X570-GAMING-X:~/terraform$ terraform plan

Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the
following symbols:
  + create

Terraform will perform the following actions:

  # yandex_vpc_network.net will be created
  + resource "yandex_vpc_network" "net" {
      + created_at                = (known after apply)
      + default_security_group_id = (known after apply)
      + folder_id                 = (known after apply)
      + id                        = (known after apply)
      + labels                    = (known after apply)
      + name                      = "prod-net"
      + subnet_ids                = (known after apply)
    }

  # yandex_vpc_subnet.subnet will be created
  + resource "yandex_vpc_subnet" "subnet" {
      + created_at     = (known after apply)
      + folder_id      = (known after apply)
      + id             = (known after apply)
      + labels         = (known after apply)
      + name           = "prod-net-subnet"
      + network_id     = (known after apply)
      + v4_cidr_blocks = [
          + "172.16.0.0/24",
        ]
      + v6_cidr_blocks = (known after apply)
      + zone           = "ru-central1-a"
    }

  # module.yc_instance_count.yandex_compute_instance.test-vm[0] will be created
  + resource "yandex_compute_instance" "test-vm" {
      + created_at                = (known after apply)
      + folder_id                 = (known after apply)
      + fqdn                      = (known after apply)
      + hostname                  = "vm-count-1"
      + id                        = (known after apply)
      + metadata                  = {
          + "ssh-keys" = <<-EOT
                centos:ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCwlnFJsYr4liY2SeyjCzSC0sr+WtxpfHIaZZNfG8/jnPrGK5x9oJ2zbJH/7/m/K0rIrPj6HTdAYLafNP0wunTj6Wu9K+WhrkXyXvU6GOqbZB5Vv8J66TLcxWRkh/yKFlTeQ41eMjCHvFfKMdlbOh+CzCJOf7+8G9hm8BU7j7RMLBhlUZaw5DSh2/mLy2MSvLZoRtmVEsgk3PpeotqSO1z4ziA/EdRgbbEzyeouMYBEgG9wtDFtFYBYiFTqlo4XC7YLd+ldQI2A5CARHMIyl9r4YHTIUYd6kOHK++FqbIw2CFGU8pLQmnsWXwVGPIcT2lIW0U/9DlHR6w+i8FEWNjX6726eO7CywlVItHHCdtZJdCBh1UH9+FslRsFLQyy78HaRe4S4TSoVeEbA+48HAvY3WaOVlQN2ioX/jVEvnc+3iLXzp3Pm2yZsPV0/D9a3quhrHZWIlu3dGF+GAxiwxtCClbm/7kw7ooHIs+YOsF5Zc1MswCUxVxrZs6jWK8ZJVdc= fcrrrnd.rostov@gmail.com
            EOT
        }
      + name                      = "prod-1-vm-count"
      + network_acceleration_type = "standard"
      + platform_id               = "standard-v1"
      + service_account_id        = (known after apply)
      + status                    = (known after apply)
      + zone                      = "ru-central1-a"

      + boot_disk {
          + auto_delete = true
          + device_name = (known after apply)
          + disk_id     = (known after apply)
          + mode        = (known after apply)

          + initialize_params {
              + block_size  = (known after apply)
              + description = (known after apply)
              + image_id    = "fd88d14a6790do254kj7"
              + name        = (known after apply)
              + size        = (known after apply)
              + snapshot_id = (known after apply)
              + type        = "network-hdd"
            }
        }

      + network_interface {
          + index              = (known after apply)
          + ip_address         = (known after apply)
          + ipv4               = true
          + ipv6               = (known after apply)
          + ipv6_address       = (known after apply)
          + mac_address        = (known after apply)
          + nat                = true
          + nat_ip_address     = (known after apply)
          + nat_ip_version     = (known after apply)
          + security_group_ids = (known after apply)
          + subnet_id          = (known after apply)
        }

      + placement_policy {
          + host_affinity_rules = (known after apply)
          + placement_group_id  = (known after apply)
        }

      + resources {
          + core_fraction = 100
          + cores         = 4
          + memory        = 8
        }

      + scheduling_policy {
          + preemptible = (known after apply)
        }
    }

  # module.yc_instance_count.yandex_compute_instance.test-vm[1] will be created
  + resource "yandex_compute_instance" "test-vm" {
      + created_at                = (known after apply)
      + folder_id                 = (known after apply)
      + fqdn                      = (known after apply)
      + hostname                  = "vm-count-2"
      + id                        = (known after apply)
      + metadata                  = {
          + "ssh-keys" = <<-EOT
                centos:ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCwlnFJsYr4liY2SeyjCzSC0sr+WtxpfHIaZZNfG8/jnPrGK5x9oJ2zbJH/7/m/K0rIrPj6HTdAYLafNP0wunTj6Wu9K+WhrkXyXvU6GOqbZB5Vv8J66TLcxWRkh/yKFlTeQ41eMjCHvFfKMdlbOh+CzCJOf7+8G9hm8BU7j7RMLBhlUZaw5DSh2/mLy2MSvLZoRtmVEsgk3PpeotqSO1z4ziA/EdRgbbEzyeouMYBEgG9wtDFtFYBYiFTqlo4XC7YLd+ldQI2A5CARHMIyl9r4YHTIUYd6kOHK++FqbIw2CFGU8pLQmnsWXwVGPIcT2lIW0U/9DlHR6w+i8FEWNjX6726eO7CywlVItHHCdtZJdCBh1UH9+FslRsFLQyy78HaRe4S4TSoVeEbA+48HAvY3WaOVlQN2ioX/jVEvnc+3iLXzp3Pm2yZsPV0/D9a3quhrHZWIlu3dGF+GAxiwxtCClbm/7kw7ooHIs+YOsF5Zc1MswCUxVxrZs6jWK8ZJVdc= fcrrrnd.rostov@gmail.com
            EOT
        }
      + name                      = "prod-2-vm-count"
      + network_acceleration_type = "standard"
      + platform_id               = "standard-v1"
      + service_account_id        = (known after apply)
      + status                    = (known after apply)
      + zone                      = "ru-central1-a"

      + boot_disk {
          + auto_delete = true
          + device_name = (known after apply)
          + disk_id     = (known after apply)
          + mode        = (known after apply)

          + initialize_params {
              + block_size  = (known after apply)
              + description = (known after apply)
              + image_id    = "fd88d14a6790do254kj7"
              + name        = (known after apply)
              + size        = (known after apply)
              + snapshot_id = (known after apply)
              + type        = "network-hdd"
            }
        }

      + network_interface {
          + index              = (known after apply)
          + ip_address         = (known after apply)
          + ipv4               = true
          + ipv6               = (known after apply)
          + ipv6_address       = (known after apply)
          + mac_address        = (known after apply)
          + nat                = true
          + nat_ip_address     = (known after apply)
          + nat_ip_version     = (known after apply)
          + security_group_ids = (known after apply)
          + subnet_id          = (known after apply)
        }

      + placement_policy {
          + host_affinity_rules = (known after apply)
          + placement_group_id  = (known after apply)
        }

      + resources {
          + core_fraction = 100
          + cores         = 4
          + memory        = 8
        }

      + scheduling_policy {
          + preemptible = (known after apply)
        }
    }

  # module.yc_instance_for_each["p1"].yandex_compute_instance.test-vm[0] will be created
  + resource "yandex_compute_instance" "test-vm" {
      + created_at                = (known after apply)
      + folder_id                 = (known after apply)
      + fqdn                      = (known after apply)
      + hostname                  = "p1-vm-foreach-1"
      + id                        = (known after apply)
      + metadata                  = {
          + "ssh-keys" = <<-EOT
                centos:ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCwlnFJsYr4liY2SeyjCzSC0sr+WtxpfHIaZZNfG8/jnPrGK5x9oJ2zbJH/7/m/K0rIrPj6HTdAYLafNP0wunTj6Wu9K+WhrkXyXvU6GOqbZB5Vv8J66TLcxWRkh/yKFlTeQ41eMjCHvFfKMdlbOh+CzCJOf7+8G9hm8BU7j7RMLBhlUZaw5DSh2/mLy2MSvLZoRtmVEsgk3PpeotqSO1z4ziA/EdRgbbEzyeouMYBEgG9wtDFtFYBYiFTqlo4XC7YLd+ldQI2A5CARHMIyl9r4YHTIUYd6kOHK++FqbIw2CFGU8pLQmnsWXwVGPIcT2lIW0U/9DlHR6w+i8FEWNjX6726eO7CywlVItHHCdtZJdCBh1UH9+FslRsFLQyy78HaRe4S4TSoVeEbA+48HAvY3WaOVlQN2ioX/jVEvnc+3iLXzp3Pm2yZsPV0/D9a3quhrHZWIlu3dGF+GAxiwxtCClbm/7kw7ooHIs+YOsF5Zc1MswCUxVxrZs6jWK8ZJVdc= fcrrrnd.rostov@gmail.com
            EOT
        }
      + name                      = "prod-1-p1-vm-foreach"
      + network_acceleration_type = "standard"
      + platform_id               = "standard-v1"
      + service_account_id        = (known after apply)
      + status                    = (known after apply)
      + zone                      = "ru-central1-a"

      + boot_disk {
          + auto_delete = true
          + device_name = (known after apply)
          + disk_id     = (known after apply)
          + mode        = (known after apply)

          + initialize_params {
              + block_size  = (known after apply)
              + description = (known after apply)
              + image_id    = "fd88d14a6790do254kj7"
              + name        = (known after apply)
              + size        = (known after apply)
              + snapshot_id = (known after apply)
              + type        = "network-hdd"
            }
        }

      + network_interface {
          + index              = (known after apply)
          + ip_address         = (known after apply)
          + ipv4               = true
          + ipv6               = (known after apply)
          + ipv6_address       = (known after apply)
          + mac_address        = (known after apply)
          + nat                = true
          + nat_ip_address     = (known after apply)
          + nat_ip_version     = (known after apply)
          + security_group_ids = (known after apply)
          + subnet_id          = (known after apply)
        }

      + placement_policy {
          + host_affinity_rules = (known after apply)
          + placement_group_id  = (known after apply)
        }

      + resources {
          + core_fraction = 100
          + cores         = 4
          + memory        = 8
        }

      + scheduling_policy {
          + preemptible = (known after apply)
        }
    }

  # module.yc_instance_for_each["p2"].yandex_compute_instance.test-vm[0] will be created
  + resource "yandex_compute_instance" "test-vm" {
      + created_at                = (known after apply)
      + folder_id                 = (known after apply)
      + fqdn                      = (known after apply)
      + hostname                  = "p2-vm-foreach-1"
      + id                        = (known after apply)
      + metadata                  = {
          + "ssh-keys" = <<-EOT
                centos:ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCwlnFJsYr4liY2SeyjCzSC0sr+WtxpfHIaZZNfG8/jnPrGK5x9oJ2zbJH/7/m/K0rIrPj6HTdAYLafNP0wunTj6Wu9K+WhrkXyXvU6GOqbZB5Vv8J66TLcxWRkh/yKFlTeQ41eMjCHvFfKMdlbOh+CzCJOf7+8G9hm8BU7j7RMLBhlUZaw5DSh2/mLy2MSvLZoRtmVEsgk3PpeotqSO1z4ziA/EdRgbbEzyeouMYBEgG9wtDFtFYBYiFTqlo4XC7YLd+ldQI2A5CARHMIyl9r4YHTIUYd6kOHK++FqbIw2CFGU8pLQmnsWXwVGPIcT2lIW0U/9DlHR6w+i8FEWNjX6726eO7CywlVItHHCdtZJdCBh1UH9+FslRsFLQyy78HaRe4S4TSoVeEbA+48HAvY3WaOVlQN2ioX/jVEvnc+3iLXzp3Pm2yZsPV0/D9a3quhrHZWIlu3dGF+GAxiwxtCClbm/7kw7ooHIs+YOsF5Zc1MswCUxVxrZs6jWK8ZJVdc= fcrrrnd.rostov@gmail.com
            EOT
        }
      + name                      = "prod-1-p2-vm-foreach"
      + network_acceleration_type = "standard"
      + platform_id               = "standard-v1"
      + service_account_id        = (known after apply)
      + status                    = (known after apply)
      + zone                      = "ru-central1-a"

      + boot_disk {
          + auto_delete = true
          + device_name = (known after apply)
          + disk_id     = (known after apply)
          + mode        = (known after apply)

          + initialize_params {
              + block_size  = (known after apply)
              + description = (known after apply)
              + image_id    = "fd88d14a6790do254kj7"
              + name        = (known after apply)
              + size        = (known after apply)
              + snapshot_id = (known after apply)
              + type        = "network-hdd"
            }
        }

      + network_interface {
          + index              = (known after apply)
          + ip_address         = (known after apply)
          + ipv4               = true
          + ipv6               = (known after apply)
          + ipv6_address       = (known after apply)
          + mac_address        = (known after apply)
          + nat                = true
          + nat_ip_address     = (known after apply)
          + nat_ip_version     = (known after apply)
          + security_group_ids = (known after apply)
          + subnet_id          = (known after apply)
        }

      + placement_policy {
          + host_affinity_rules = (known after apply)
          + placement_group_id  = (known after apply)
        }

      + resources {
          + core_fraction = 100
          + cores         = 4
          + memory        = 8
        }

      + scheduling_policy {
          + preemptible = (known after apply)
        }
    }

Plan: 6 to add, 0 to change, 0 to destroy.

────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────

Note: You didn't use the -out option to save this plan, so Terraform can't guarantee to take exactly these actions if you
run "terraform apply" now.

```
